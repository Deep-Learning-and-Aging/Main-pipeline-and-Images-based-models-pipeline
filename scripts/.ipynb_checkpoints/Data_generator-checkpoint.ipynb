{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read parameters from command\n",
    "image_type = 'Liver'\n",
    "target = 'Age'\n",
    "model_name = 'DenseNet121'\n",
    "optimizer_name = 'Adam'\n",
    "learning_rate = 0.01\n",
    "\n",
    "#regularization: start with zero regularization. After good training performance AND overfitting is confirmed, use regularization.\n",
    "lam=0.0 #regularization: weight shrinking\n",
    "dropout_rate=0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###load libraries\n",
    "\n",
    "#read and write\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import tarfile\n",
    "import shutil\n",
    "import pyreadr\n",
    "import csv\n",
    "\n",
    "#maths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from math import sqrt\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "#images\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.ndimage import shift, rotate\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "#miscellaneous\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gc\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Flatten, Dense, Activation, Input, Reshape, BatchNormalization, InputLayer, Dropout, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Conv3D, MaxPooling3D, GlobalAveragePooling2D, LSTM\n",
    "from keras.models import Sequential, Model, model_from_json, clone_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, TensorBoard, ModelCheckpoint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "#from tensorflow.keras.metrics import Recall, Precision, AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to put in helper file\n",
    "def append_ext(fn):\n",
    "    return fn+\".jpg\"\n",
    "\n",
    "def generate_base_model(model_name, lam, dropout_rate, import_weights):\n",
    "    if model_name in ['VGG16', 'VGG19']:\n",
    "        if model_name == 'VGG16':\n",
    "            from keras.applications.vgg16 import VGG16\n",
    "            base_model = VGG16(include_top=False, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'VGG19':\n",
    "            from keras.applications.vgg19 import VGG19\n",
    "            base_model = VGG19(include_top=False, weights=import_weights, input_shape=(224,224,3))\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "        x = Dropout(dropout_rate)(x) \n",
    "    elif model_name in ['MobileNet', 'MobileNetV2']:\n",
    "        if model_name == 'MobileNet':\n",
    "            from keras.applications.mobilenet import MobileNet\n",
    "            base_model = MobileNet(include_top=False, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'MobileNetV2':\n",
    "            from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "            base_model = MobileNetV2(include_top=False, weights=import_weights, input_shape=(224,224,3))\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif model_name in ['DenseNet121', 'DenseNet169', 'DenseNet201']:\n",
    "        if model_name == 'DenseNet121':\n",
    "            from keras.applications.densenet import DenseNet121\n",
    "            base_model = DenseNet121(include_top=True, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'DenseNet169':\n",
    "            from keras.applications.densenet import DenseNet169\n",
    "            base_model = DenseNet169(include_top=True, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'DenseNet201':\n",
    "            from keras.applications.densenet import DenseNet201\n",
    "            base_model = DenseNet201(include_top=True, weights=import_weights, input_shape=(224,224,3))            \n",
    "        base_model = Model(base_model.inputs, base_model.layers[-2].output)\n",
    "        x = base_model.output\n",
    "    elif model_name in ['NASNetMobile', 'NASNetLarge']:\n",
    "        if model_name == 'NASNetMobile':\n",
    "            from keras.applications.nasnet import NASNetMobile\n",
    "            base_model = NASNetMobile(include_top=True, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'NASNetLarge':\n",
    "            from keras.applications.nasnet import NASNetLarge\n",
    "            base_model = NASNetLarge(include_top=True, weights=import_weights, input_shape=(331,331,3))\n",
    "        base_model = Model(base_model.inputs, base_model.layers[-2].output)\n",
    "        x = base_model.output\n",
    "    elif model_name == 'Xception':\n",
    "        from keras.applications.xception import Xception\n",
    "        base_model = Xception(include_top=False, weights=import_weights, input_shape=(299,299,3))\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif model_name == 'InceptionV3':\n",
    "        from keras.applications.inception_v3 import InceptionV3\n",
    "        base_model = InceptionV3(include_top=False, weights=import_weights, input_shape=(299,299,3))\n",
    "        x = base_model.output        \n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif model_name == 'InceptionResNetV2':\n",
    "        from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "        base_model = InceptionResNetV2(include_top=False, weights=import_weights, input_shape=(299,299,3))\n",
    "        x = base_model.output        \n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    return x, base_model.input\n",
    "\n",
    "def complete_architecture(x, input_shape, lam, dropout_rate):\n",
    "    x = Dense(1024, activation='selu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(512, activation='selu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(128, activation='selu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(64, activation='selu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(1, activation='linear')(x)\n",
    "    model = Model(inputs=input_shape, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "def R_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "  \n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def set_learning_rate(model, optimizer_name, learning_rate):\n",
    "    opt = globals()[optimizer_name](lr=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error', metrics=[R_squared, root_mean_squared_error])\n",
    "    \n",
    "def initialize_history():\n",
    "    HISTORY = {}\n",
    "    for metric in ['loss'] + metrics:\n",
    "        for fold in folds_tune:\n",
    "            if(fold=='train'):\n",
    "                HISTORY[metric] = []\n",
    "            else:\n",
    "                HISTORY[fold + '_' + metric] = []\n",
    "    return HISTORY\n",
    "\n",
    "def update_history(HISTORY, history):\n",
    "    keys = history.history.keys()\n",
    "    for key in keys:\n",
    "        HISTORY[key] = HISTORY[key] + history.history[key]\n",
    "    return HISTORY\n",
    "    \n",
    "def plot_training(HISTORY, version):\n",
    "    keys = history.history.keys()\n",
    "    fig, axs = plt.subplots(1, int(len(keys)/2), sharey=False, sharex=True)\n",
    "    fig.set_figwidth(15)\n",
    "    fig.set_figheight(5)\n",
    "    epochs = np.array(range(len(HISTORY[metrics[0]])))\n",
    "    for i, metric in enumerate(['loss'] + metrics):\n",
    "        for key in [key for key in keys if metric in key][::-1]:\n",
    "            axs[i].plot(epochs, HISTORY[key])\n",
    "        axs[i].legend(['Training ' + metric, 'Validation ' + metric])\n",
    "        axs[i].set_title(metric + ' = f(Epoch)')\n",
    "        axs[i].set_xlabel('Epoch')\n",
    "        axs[i].set_ylabel(metric)\n",
    "        axs[i].set_ylim((-0.2, 1.1))\n",
    "    #save figure as pdf\n",
    "    fig.savefig(\"../figures/Training_\" + version + '.pdf', bbox_inches='tight')\n",
    "    \n",
    "def save_model_weights(model, version):\n",
    "    model.save_weights(path_store + \"model_weights_\" + version + \".h5\")\n",
    "    print(\"Model's best weights for \"+ version + \" were saved.\")\n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def generate_predictions_and_performances(model, GENERATORS, STEPSIZES):\n",
    "    PREDS={}\n",
    "    PERFORMANCES={}\n",
    "    for fold in folds:\n",
    "        generator = GENERATORS[fold]\n",
    "        generator.reset()\n",
    "        PREDS[fold]=model.predict_generator(generator, STEPSIZES[fold], verbose=1)\n",
    "    for metric in metrics:\n",
    "            PERFORMANCES[metric] = {}\n",
    "            for fold in folds:\n",
    "                PERFORMANCES[metric][fold] = metric_functions[metric](DATA_FEATURES[fold][target], PREDS[fold])\n",
    "    return PREDS, PERFORMANCES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters to put in helper file\n",
    "folds = ['train', 'val', 'test']\n",
    "folds_tune = ['train', 'val']\n",
    "models_names = ['VGG16', 'VGG19', 'MobileNet', 'MobileNetV2', 'DenseNet121', 'DenseNet169', 'DenseNet201', 'NASNetMobile', 'NASNetLarge', 'Xception', 'InceptionV3', 'InceptionResNetV2']\n",
    "images_sizes = ['224', '299', '331']\n",
    "metrics = ['R_squared', 'root_mean_squared_error']\n",
    "main_metrics = dict.fromkeys(['Age'], 'R_squared')\n",
    "main_metrics.update(dict.fromkeys(['Sex'], 'AUC'))\n",
    "metric_functions = {'R_squared':r2_score, 'root_mean_squared_error':rmse}\n",
    "\n",
    "#define dictionary to resize the images to the right size depending on the model\n",
    "input_size_models = dict.fromkeys(['VGG16', 'VGG19', 'MobileNet', 'MobileNetV2', 'DenseNet121', 'DenseNet169', 'DenseNet201', 'NASNetMobile'], 224)\n",
    "input_size_models.update(dict.fromkeys(['Xception', 'InceptionV3', 'InceptionResNetV2'], 299))\n",
    "input_size_models.update(dict.fromkeys(['NASNetLarge'], 331))\n",
    "\n",
    "#define dictionaries to format the text\n",
    "dict_folds={'train':'Training', 'val':'Validation', 'test':'Testing'}\n",
    "\n",
    "#define paths\n",
    "if '/Users/Alan/' in os.getcwd():\n",
    "    os.chdir('/Users/Alan/Desktop/Aging/Medical_Images/scripts/')\n",
    "    path_store = '../data/'\n",
    "    path_compute = '../data/'\n",
    "else:\n",
    "    os.chdir('/n/groups/patel/Alan/Aging/Medical_Images/scripts/')\n",
    "    path_store = '../data/'\n",
    "    path_compute = '/n/scratch2/al311/Aging/Medical_Images/data/'\n",
    "\n",
    "if image_type == 'Liver':\n",
    "    dir_images = path_store + '../../../../uk_biobank/main_data_52887/Liver/Liver_20204/'\n",
    "else:\n",
    "    sys.exit(\"Error. Image type not available\")\n",
    "\n",
    "#model\n",
    "image_size = input_size_models[model_name]\n",
    "import_weights = 'imagenet' #choose between None and 'imagenet'\n",
    "\n",
    "#compiler\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "continue_training = True\n",
    "main_metric = main_metrics[target]\n",
    "version = target + '_' + image_type + '_' + model_name + '_' + optimizer_name + '_' + str(learning_rate) + '_' + str(lam) + '_' + str(dropout_rate) + '_' + str(batch_size)\n",
    "\n",
    "#postprocessing\n",
    "boot_iterations=10000\n",
    "\n",
    "#set parameters\n",
    "random.seed(0)\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version :  1.13.1\n",
      "Build with Cuda :  True\n",
      "Gpu available :  True\n"
     ]
    }
   ],
   "source": [
    "#print versions and info\n",
    "print('tensorflow version : ', tf.__version__)\n",
    "print('Build with Cuda : ', tf.test.is_built_with_cuda())\n",
    "print('Gpu available : ', tf.test.is_gpu_available())\n",
    "#print('Available ressources : ', tf.config.experimental.list_physical_devices())\n",
    "config = tf.ConfigProto()\n",
    "#device_count = {'GPU': 1, 'CPU': mp.cpu_count() },log_device_placement =  True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess= tf.Session(config = config)\n",
    "K.set_session(session= sess)\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore which variables are present in the features dataset\n",
    "#data_features = pd.read_csv(path_store + \"../../../../uk_biobank/main_data_52887/ukb37397.csv\", nrows=1)\n",
    "#data_features.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the selected features\n",
    "data_features = pd.read_csv(path_store + \"../../../../uk_biobank/main_data_52887/ukb37397.csv\", usecols=['eid','21003-0.0','31-0.0', '22414-2.0'])\n",
    "data_features.columns = ['eid', 'Sex','Age', 'Data_quality']\n",
    "data_features['eid'] =  data_features['eid'].astype(str)\n",
    "data_features = data_features.set_index('eid', drop=False)\n",
    "#remove the samples for which the liver data is low quality\n",
    "data_features = data_features[data_features['Data_quality']!=np.nan]\n",
    "data_features = data_features.drop(\"Data_quality\", axis=1)\n",
    "#get rid of samples with NAs\n",
    "data_features = data_features.dropna()\n",
    "#list the samples' ids for which liver images are available\n",
    "all_files = os.listdir(dir_images)\n",
    "all_ids = [file.split(\".\", maxsplit=1)[0] for file in all_files]\n",
    "data_features = data_features.loc[all_ids]\n",
    "files = data_features.index.values\n",
    "#save the features\n",
    "data_features.to_csv(path_store + \"data_features_\" + version + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../data/data_features.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9c35357a6787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#generate ids and image generators for train, val, test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_store\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"data_features.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdata_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eid\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/app/python/3.6.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/app/python/3.6.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/app/python/3.6.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/app/python/3.6.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/app/python/3.6.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4025)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8031)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../data/data_features.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#define data generators\n",
    "datagen_train=ImageDataGenerator(rescale=1./255.)\n",
    "datagen_test = ImageDataGenerator(rescale=1./255.)\n",
    "class_mode_train = \"raw\"\n",
    "class_mode_test = None\n",
    "\n",
    "#generate ids and image generators for train, val, test\n",
    "folds = ['train', 'val', 'test']\n",
    "data_features = pd.read_csv(path_store + \"data_features_\" + version + \".csv\")\n",
    "data_features['eid'] = data_features['eid'].apply(str)\n",
    "data_features[\"eid\"] = data_features[\"eid\"].apply(append_ext)\n",
    "data_features.set_index('eid', drop=False)\n",
    "ids = data_features.index.values.copy()\n",
    "np.random.shuffle(ids)\n",
    "percent_train = 0.8\n",
    "percent_val = 0.1\n",
    "n_discard = len(ids)%batch_size\n",
    "n_limit_train = math.ceil(len(ids)/batch_size*percent_train)*batch_size\n",
    "n_limit_val = math.ceil(len(ids)/batch_size*(percent_train+percent_val))*batch_size\n",
    "n_limit_test = len(ids)-n_discard\n",
    "\n",
    "#split IDs\n",
    "IDs={}\n",
    "IDs['train'] = ids[:n_limit_train]\n",
    "IDs['val'] = ids[n_limit_train:n_limit_val]\n",
    "IDs['test'] = ids[n_limit_val:n_limit_test]\n",
    "\n",
    "#compute values for scaling of Age\n",
    "fold = 'train'\n",
    "idx = np.where(np.isin(data_features.index.values, IDs['train']))[0]\n",
    "data_features_train = data_features.iloc[idx,:]\n",
    "Age_mean = data_features_train['Age'].mean()\n",
    "Age_std = data_features_train['Age'].std()\n",
    "\n",
    "#split data_features\n",
    "indices={}\n",
    "DATA_FEATURES = {}\n",
    "GENERATORS = {}\n",
    "STEP_SIZES = {}\n",
    "for fold in folds:\n",
    "    indices[fold] = np.where(np.isin(data_features.index.values, IDs[fold]))[0]\n",
    "    data_features_fold = data_features.iloc[indices[fold],:]\n",
    "    data_features_fold.to_csv(path_store + \"data_features_\" + fold + \".csv\")\n",
    "    data_features_fold['Age'] = (data_features_fold['Age']-Age_mean)/Age_std\n",
    "    if fold == 'test':\n",
    "        datagen=datagen_test\n",
    "        class_mode = class_mode_test\n",
    "    else:\n",
    "        datagen=datagen_train\n",
    "        class_mode = class_mode_train\n",
    "    \n",
    "    #define data generator\n",
    "    generator_fold = datagen.flow_from_dataframe(\n",
    "        dataframe=data_features_fold,\n",
    "        directory=dir_images,\n",
    "        x_col=\"eid\",\n",
    "        y_col=target,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        class_mode=\"raw\",\n",
    "        target_size=(image_size,image_size))\n",
    "    \n",
    "    #assign variables to their names\n",
    "    DATA_FEATURES[fold] = data_features_fold\n",
    "    GENERATORS[fold] = generator_fold\n",
    "    STEP_SIZES[fold] = generator_fold.n//generator_fold.batch_size\n",
    "\n",
    "#define the model\n",
    "x, base_model_input = generate_base_model(model_name=model_name, lam=lam, dropout_rate=dropout_rate, import_weights=import_weights)\n",
    "model = complete_architecture(x=x, input_shape=base_model_input, lam=lam, dropout_rate=dropout_rate)\n",
    "\n",
    "#initialise history\n",
    "HISTORY = initialize_history()\n",
    "\n",
    "#take subset to debunk\n",
    "#DATA_FEATURES['train'] = DATA_FEATURES['test']\n",
    "#GENERATORS['train'] = GENERATORS['test']\n",
    "#STEP_SIZES['train'] = STEP_SIZES['test']\n",
    "#n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(re-)set the learning rate\n",
    "set_learning_rate(model, optimizer_name, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights to continue training\n",
    "path_weights = path_store + \"model_weights_\" + version + \".h5\"\n",
    "if continue_training & os.path.exists(path_weights):\n",
    "    print(\"loading previous model's weights\")\n",
    "    #load weights\n",
    "    model.load_weights(path_weights)\n",
    "    #load previous best performance\n",
    "    json_file = open(path_store + 'Performance_' + version + '.json', 'r')\n",
    "    best_perf = json_file.read()\n",
    "    json_file.close()\n",
    "    N_epochs = best_perf['N_epochs']\n",
    "    max_perf_val = best_perf[main_metric]['val']\n",
    "else:\n",
    "    N_epochs = 0\n",
    "    max_perf_val = -np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "while True:\n",
    "    history = model.fit_generator(generator=GENERATORS['train'],\n",
    "                    steps_per_epoch=STEP_SIZES['train'],\n",
    "                    validation_data=GENERATORS['val'],\n",
    "                    validation_steps=STEP_SIZES['val'],\n",
    "                    use_multiprocessing = True,\n",
    "                    epochs=n_epochs)\n",
    "    #compute performances\n",
    "    N_epochs += n_epochs\n",
    "    HISTORY = update_history(HISTORY, history)\n",
    "    plot_training(HISTORY, version)\n",
    "    PREDS, PERF = generate_predictions_and_performances(model, GENERATORS, STEP_SIZES)\n",
    "    print('N_epochs = ' + str(N_epochs))\n",
    "    print('Performance summary: ')\n",
    "    print(PERF)\n",
    "    if np.max(history.history['val_' + main_metric]) > max_perf_val:\n",
    "        print('A better model was found in the middle of the epoch batch with validation ' + main_metric + ' = ' + str(np.max(history.history['val_' + main_metric])))\n",
    "    if np.max(PERF[main_metric]['val']) > max_perf_val:\n",
    "        max_metric_val = np.max(PERF[main_metric]['val'])\n",
    "        save_model_weights(model, version)\n",
    "        to_save = PERF.copy()\n",
    "        to_save['version'] = version\n",
    "        to_save['N_epochs'] = N_epochs\n",
    "        json.dump(to_save, open(path_store + 'Performance_' + version + '.json','w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
